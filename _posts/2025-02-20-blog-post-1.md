---
title: 'my first PR'
date: 2025-02-20
permalink: /posts/2025/02-blog-post-7/
tags:
  - Github
---

ã€Œæ˜¥ã®é¢¨ ï¼Œ å¿™ã—ãæ—¥ã€… ï¼Œ è•¾ã®å¤¢ã€

Today I submitted my first PR and it was merged by the author! Although it was just a small version update, I still feel quite accomplished! ðŸŽ‰

Recently, Iâ€™ve encountered two interesting projects - one is the YOLO backend for label-studio (using local YOLO models for automatic annotation), and the other is FastSAM (similar to SAM but 50 times faster). âš¡

Iâ€™ve been wondering if we could use traditional CNN as a Prompt input for ViT models like SAM. For example, using mediapipe hand keypoint detection and feeding the keypoint coordinates as Prompts into the SAM model - what scenarios could this be used for? Maybe for open-world interactions? Though SAMâ€™s real-time performance isnâ€™t great. Still, itâ€™s quite innovative, or we could crop around the hand area to reduce input size, which would speed things up significantly. ðŸ¤”

The sad reality of Prompt engineering! As undergraduates, we neither have the technical expertise to design new models nor the computing power to train large models. We can only use Prompts to try to elicit emergent properties from models. It feels a bit like cyberpunk corporate monopoly. ðŸ’­

On the bright side, Grok3 has finally been released. For models supporting web search, I think the most important thing is whether they can actively search for high-quality English content and translate it when given Chinese queries. Currently, Grok3 and gemini-2.0-thinking-flash have achieved this, and I hope these experimental models maintain this feature. If the official version continues like Grok2 to only search Chinese content for Chinese queries, it would be neither fair nor practical. â˜ ï¸


> Spring breeze ï¼ŒBusy days ï¼Œ Dreams of buds ðŸŒ¿
