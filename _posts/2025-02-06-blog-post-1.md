---
title: 'Performance Bottleneck'
date: 2025-02-06
permalink: /posts/2025/02-blog-post-3/
tags:
  - LLM
  - Kaggle
  - Sad
---

「晴れ間すぐ曇り， また雨の音 ， 行手山」

After learning the theory of LLM, the **truly boring and painful** part is applying it to fine-tuning tasks. I hope to use a humor joke dataset (organized by myself) to enable the model to answer wisely like the discontinued grok fun mode. 🤖

First is the unstable Kaggle. *P100 often encounters the problem of running out of memory*, and then 2 T4s, although with enough memory, are still too slow. 🐌

Secondly, the fine-tuning failed. LoRA fine-tuning **gemma-2-2b-it** with only 0.12% of the parameters, 9k lines of data for 3 epochs, takes more than 3 hours. The most torturous thing is that the loss is always stuck at 0.7, but I am reluctant to stop in the middle of training, expecting a miracle that I know will not happen. 😔

Finally, I also tried LoRA fine-tuning *gpt2-medium* with 1.2% parameters, and used another carefully crafted 5k line dataset. Although the loss was stuck at 2.2, I did make this model more stupid and more irritable. It is hard to imagine that this is a fine-tuned model that grew up in the jokes of Mark Twain and Hitchcock. 

I have been dizzy with LLM these two days, but it is finally coming to an end. Next, I have to do the **gesture detection** project that has been delayed until now, which is even more annoying... 🤯 I am wondering whether to continue using yolo-seg or study some new algorithms. *Transformer definitely cannot run in Raspberry Pi*. Well, this is also one of the reasons for my sadness. Vision Transformer is not only slow, but also has a lower mAP than yolo. The most important thing is the training time and dataset!

> "Clear sky, brief moment, Quickly clouds, again rain sound, Path ahead, mountain." 🗻